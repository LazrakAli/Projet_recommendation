{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5b/qs0n20853dj5x8g98kk2c36h0000gn/T/ipykernel_97832/3226778929.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/martin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/martin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('details.csv')\n",
    "df_avis = pd.read_csv('avis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text, language='french')\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['processed_description'] = df['description'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extraction\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['processed_description'])\n",
    "\n",
    "# Similarity Measure\n",
    "cosine_sim_description = cosine_similarity(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'categories' column and explode it into separate rows\n",
    "split_categories = df['categories'].str.split(' | ').explode()\n",
    "\n",
    "# Remove leading/trailing whitespace and get unique categories\n",
    "unique_categories = split_categories.str.strip().unique()\n",
    "\n",
    "# Convert the result into a list\n",
    "unique_category_list = list(unique_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame and it has a 'categories' column\n",
    "df['categories'] = df['categories'].fillna('No Category').apply(lambda x: x.split('|'))\n",
    "\n",
    "# Explode the categories into separate rows for each category per game\n",
    "split_categories = df.explode('categories')\n",
    "\n",
    "# Remove leading/trailing whitespace in categories\n",
    "split_categories['categories'] = split_categories['categories'].str.strip()\n",
    "\n",
    "# Get unique categories (no need to explicitly convert to list for MultiLabelBinarizer)\n",
    "unique_categories = split_categories['categories'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset 'categories' column to lists for MultiLabelBinarizer compatibility\n",
    "df['categories'] = df['categories'].apply(lambda categories: [category.strip() for category in categories])\n",
    "\n",
    "# Initialize and fit the MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "categories_matrix = mlb.fit_transform(df['categories'])\n",
    "\n",
    "# Create a DataFrame for the encoded categories\n",
    "categories_df = pd.DataFrame(categories_matrix, columns=mlb.classes_)\n",
    "\n",
    "\n",
    "# Calculate cosine similarity matrix\n",
    "cosine_sim_categories = cosine_similarity(categories_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommend(title, df, category_cosine_sim, description_cosine_sim, weights={'category': 0.5, 'description': 0.5}):\n",
    "    # Assume both similarity matrices are aligned and have the same shape\n",
    "    idx = df.index[df['full_title'] == title].tolist()[0]\n",
    "    \n",
    "    # Get similarity scores from both systems\n",
    "    category_scores = category_cosine_sim[idx]\n",
    "    description_scores = description_cosine_sim[idx]\n",
    "    \n",
    "    # Combine scores\n",
    "    combined_scores = weights['category'] * category_scores + weights['description'] * description_scores\n",
    "    \n",
    "    # Rank games based on combined scores\n",
    "    ranked_indices = combined_scores.argsort()[::-1][1:11]  # Exclude the first one (the game itself)\n",
    "    \n",
    "    # Return the top 10 recommended titles\n",
    "    return df['full_title'].iloc[ranked_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights={'category':0.4, 'description':0.6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5699                   Babel (2013)\n",
       "7004             Tetris Link (2012)\n",
       "16326                   Gods (2001)\n",
       "5716              Antique II (2014)\n",
       "2554              Antique II (2014)\n",
       "18522    Alexander the Great (2005)\n",
       "14689           Gloria Mundi (2006)\n",
       "12716              Beim Zeus (1997)\n",
       "17537                Arkaios (2004)\n",
       "16003                 Angkor (2005)\n",
       "Name: full_title, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hybrid_recommend('Babel (2013)',df,cosine_sim_categories, cosine_sim_description,weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
