{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommenders 2 -- (45min) \n",
    "\n",
    "## Goals of this practical:\n",
    "\n",
    "1. Understand/Implement two common ranking metrics (~15min)\n",
    "2. See how SVD model perform w/ respect to those metrics (~15min)\n",
    "3. Understand the difference between implicit/explicit CF (~5min)\n",
    "3. Know and implement a really simple ranking baseline (~5min)\n",
    "4. Use a library to build an implicit recommender sysem (~5min)\n",
    "\n",
    "\n",
    "\n",
    "## Data used : [smallest movie-lens dataset](https://grouplens.org/datasets/movielens/)\n",
    "\n",
    "In this practical we use a small dataset of user ratings on movies. Specifically, we treat the dataset as list of $(user,item,rating)$ triplets.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment this to install required packages if needed (and restart kernel !)\n",
    "#! pip install --upgrade pandas\n",
    "#! pip install --upgrade seaborn\n",
    "#! pip install --upgrade scikit-surprise\n",
    "#! pip install --upgrade numpy\n",
    "#! pip install --upgrade lightfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # For array things\n",
    "import pandas as pd # For handling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data  & creating train/test (same as before)\n",
    "\n",
    "=> We still consider 20% as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>title_review</th>\n",
       "      <th>note</th>\n",
       "      <th>title</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Monsieur Guillaume</td>\n",
       "      <td>Voyages sur les ailes des papillons</td>\n",
       "      <td>8</td>\n",
       "      <td>Mariposas</td>\n",
       "      <td>Lorsque le jeu est jeu, bon, réflexif, joli po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>morlockbob</td>\n",
       "      <td>le festival de Michoacan</td>\n",
       "      <td>7</td>\n",
       "      <td>Mariposas</td>\n",
       "      <td>Comment continuer après un mega hit ? Simpleme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>SwatSh</td>\n",
       "      <td>Vivez la migration des monarques</td>\n",
       "      <td>7</td>\n",
       "      <td>Mariposas</td>\n",
       "      <td>Vin d'jeu: Avec Mariposas, Elizabeth Hargrave ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Timi JeuxATheme</td>\n",
       "      <td>Bon</td>\n",
       "      <td>8</td>\n",
       "      <td>Mariposas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>prunelles</td>\n",
       "      <td>Envolez-moi</td>\n",
       "      <td>9</td>\n",
       "      <td>Mariposas</td>\n",
       "      <td>Très joli bijou que ce jeu-là ! Le matériel, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0              author                         title_review  note  \\\n",
       "0           0  Monsieur Guillaume  Voyages sur les ailes des papillons     8   \n",
       "1           1          morlockbob             le festival de Michoacan     7   \n",
       "2           2              SwatSh     Vivez la migration des monarques     7   \n",
       "3           3     Timi JeuxATheme                                  Bon     8   \n",
       "4           4           prunelles                          Envolez-moi     9   \n",
       "\n",
       "       title                                            comment  \n",
       "0  Mariposas  Lorsque le jeu est jeu, bon, réflexif, joli po...  \n",
       "1  Mariposas  Comment continuer après un mega hit ? Simpleme...  \n",
       "2  Mariposas  Vin d'jeu: Avec Mariposas, Elizabeth Hargrave ...  \n",
       "3  Mariposas                                                NaN  \n",
       "4  Mariposas  Très joli bijou que ce jeu-là ! Le matériel, l...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv(\"./BDD/avis.csv\")\n",
    "#ratings = ratings.drop(['url','date_published','treated', 'review_href'], axis=1)\n",
    "ratings.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indexes,test_indexes = [],[]\n",
    "\n",
    "for index in range(len(ratings)):\n",
    "    if index%5 == 0:\n",
    "        test_indexes.append(index)\n",
    "    else:\n",
    "        train_indexes.append(index)\n",
    "\n",
    "train_ratings = ratings.iloc[train_indexes].copy()\n",
    "test_ratings = ratings.iloc[test_indexes].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Collaborative Filtering and Ranking Metrics\n",
    "\n",
    "Most of the time recommender systems present $k$ items to the users. Therefore, recommenders are often evaluated by how many relevant items are in their $k$ set (using ranking metrics).\n",
    "\n",
    "Different ranking methods exists such as:\n",
    "- [Mean Reciprocal Rank](http://en.wikipedia.org/wiki/Mean_reciprocal_rank) \n",
    "- [normalized Discounted Cumulative Gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Reciprocal Rank :\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Mean_reciprocal_rank):\n",
    "\n",
    "> The mean reciprocal rank is a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer: 1 for first place, $\\frac{1}{2}$ for second place, $\\frac{1}{3}$ for third place and so on. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q\n",
    "\n",
    "$$ MRR = \\frac{1}{|Q|}\\sum^{|Q|}_{i=1}\\frac{1}{\\text{rank}_i} $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric can be used to assess the mean rank of relevant suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Implement MRR\n",
    "\n",
    "To compute MRR, we need relevance lists which encodes whether or not items are relevant: \n",
    "\n",
    "- it's a list of 0's or 1's respectively meaning 'not relevant' and 'relevant'\n",
    "- In the MRR case, we're just looking for the 1st relevant item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4583333333333333"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_list = [[0,0,1],[0,1,0],[1,0,0],[0,0,0]]\n",
    "\n",
    "def rr(list_items):\n",
    "    relevant_indexes = np.asarray(list_items).nonzero()[0]\n",
    "    \n",
    "    if len(relevant_indexes) > 0:\n",
    "        return 1/(relevant_indexes[0]+1) # arrays are indexed from 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def mrr(list_list_items):\n",
    "    return np.mean([rr(list_item) for list_item in list_list_items])\n",
    "\n",
    "mrr(test_list) #0.4583333333333333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (normalized) Discounted Cumulative Gain:\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "\n",
    "> Discounted cumulative gain (DCG) is a measure of ranking quality. In information retrieval, it is often used to measure effectiveness of web search engine algorithms or related applications. Using a graded relevance scale of documents in a search-engine result set, DCG measures the usefulness, or gain, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower ranks.\n",
    "\n",
    "$$DCG_p = \\sum^p_{i=1}\\frac{rel_i}{\\log_2{(i+1)}} = rel_1 + \\sum^p_{i=2}\\frac{rel_i}{\\log_2{(i+1)}}$$\n",
    "\n",
    ">Two assumptions are made in using DCG and its related measures.\n",
    "- Highly relevant documents are more useful when appearing earlier in a search engine result list (have higher ranks)\n",
    "- Highly relevant documents are more useful than marginally relevant documents, which are in turn more useful than non-relevant documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Implement DCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "4.2618595071429155\n"
     ]
    }
   ],
   "source": [
    "# The dcg@k is the sum of the relevance, penalized gradually\n",
    "def dcg_at_k(r, k):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        \n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        \n",
    "    return 0.\n",
    "\n",
    "# test values\n",
    "# r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "# dcg_at_k(r, 1) => 3.0\n",
    "# dcg_at_k(r, 2) => 4.2618595071429155\n",
    "r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "print(dcg_at_k(r, 1))\n",
    "print(dcg_at_k(r, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Result lists vary in length. Comparing performance from one query to the next cannot be consistently achieved using DCG alone, so the cumulative gain at each position for a chosen value of p should be normalized. The normalized discounted cumulative gain, or nDCG, is computed as: \n",
    "\n",
    "$$ nDCG_p = \\frac{DCG_p}{IDCG_p} $$\n",
    "\n",
    "> Where IDCG_p is the maximum possible DCG through position p, also called Ideal DCG (IDCG). It is obtained by sorting all relevant documents in the corpus by their relative relevance and computing the DCG:\n",
    "\n",
    "$$ IDCG_p = max(DCG_p) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Implement NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7942854176010882"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# And it's normalized version\n",
    "def ndcg_at_k(r, k):\n",
    "    \"\"\"\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "    \"\"\"\n",
    "    dcg_max =  dcg_at_k(sorted(r)[::-1],k) \n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / dcg_max\n",
    "\n",
    "# test values\n",
    "# r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "# ndcg_at_k(r, 1) => 1.0\n",
    "# ndcg_at_k(r, 4) => 0.794285\n",
    "    \n",
    "r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]    \n",
    "print(ndcg_at_k(r,1))\n",
    "ndcg_at_k(r, 4)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, let's see how SVD performs on these metrics:\n",
    "\n",
    "Here, we're not in the same evaluation framework as in the last session. The goal for our recommender is to present $k$ items to the users instead of simply scoring $(user,item)$ pairs. We can still use previous models if we consider the predicted rating as a way to rank item to present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  To make a recommender we also need two `{userId: [movieId, movieId, ...] }` dictionnaries:\n",
    "\n",
    "- `already_seen`: Items that were already seen by users. This is for training and not recommending them again\n",
    "- `ground_truth`: Items that will be seen and liked (rating >= 5) by users. This is our ground truth to evaluate our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_seen = (\n",
    "    train_ratings \n",
    "    .groupby(\"author\")[\"title\"]\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    "    )\n",
    "\n",
    "ground_truth = (\n",
    "    test_ratings[test_ratings.note >= 8] \n",
    "    .groupby(\"author\")[\"title\"]\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We also need the set of all items that can be recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recommender system will have to pick a few items from 3306 possible items\n"
     ]
    }
   ],
   "source": [
    "existing_items = set(train_ratings[\"title\"].unique())\n",
    "print(\"The recommender system will have to pick a few items from\",len(existing_items),\"possible items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Ok, now, let's make a quick surprise SVD recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x24fd9994810>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "data = Dataset.load_from_df(train_ratings[['author', 'title', 'note']], Reader(rating_scale=(0, 10)))\n",
    "model = SVD()\n",
    "\n",
    "# training a quick model\n",
    "model.fit(data.build_full_trainset())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to make mse predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 3.4309851890243444 -- MAE: 1.424524244021255\n"
     ]
    }
   ],
   "source": [
    "def svd_rating_pred(user_item):\n",
    "    user = user_item[\"author\"]\n",
    "    item = user_item[\"title\"]\n",
    "    \n",
    "    prediction = model.predict(user,item)\n",
    "    \n",
    "    return prediction.est\n",
    "\n",
    "test_ratings[\"svd_prediction\"] = test_ratings[[\"author\",\"title\"]].apply(svd_rating_pred,axis=1) \n",
    "\n",
    "mse = ((test_ratings[\"note\"] - test_ratings[\"svd_prediction\"])**2).mean()\n",
    "mae = ((test_ratings[\"note\"] - test_ratings[\"svd_prediction\"]).abs()).mean()\n",
    "\n",
    "print(f\"MSE: {mse} -- MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (todo) function to make predictions given `(surprise model,user,item)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_rating_pred(model,user,item):\n",
    "    prediction = model.predict(user,item)\n",
    "    return prediction.est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create the relevance list for our MRR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_rel = []\n",
    "\n",
    "for user,will_see in ground_truth.items():\n",
    "    rel_list = []\n",
    "    will_see = set(will_see)\n",
    "    has_seen = set(already_seen.get(user, []))\n",
    "    can_see = [(mid,model_rating_pred(model,user,mid)) for mid in existing_items - has_seen]\n",
    "    \n",
    "    \n",
    "    for movie,score in reversed(sorted(can_see,key=lambda x:x[1])):\n",
    "        if movie in will_see:\n",
    "            rel_list.append(1)\n",
    "            break\n",
    "        else:\n",
    "            rel_list.append(0)        \n",
    "    rel_list[-1] = 1 # when no relevant item exist\n",
    "    list_of_rel.append(rel_list)\n",
    "    \n",
    "\n",
    "svd_mrr = mrr(list_of_rel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On average, the 27th proposed item is relevant (on 3306)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"On average, the {int(round(1/svd_mrr,0))}th proposed item is relevant (on {len(existing_items)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our result is the following :\n",
    "> 'On average, the 9th proposed item is relevant (on 8970)'\n",
    "\n",
    "So, on average, if you show around 10 items to someone, there will be at least one which will be relevant: not bad for a guess on 8970 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need something better to compare though:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A good enough implicit baseline: popular items\n",
    "What if you only show popular items to people ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) let's find the popular items:\n",
    "\n",
    "- By popular we mean the \"most\" rated movies (the one with the more rating)\n",
    "\n",
    "should be something along this: \n",
    "[318,\n",
    " 356,\n",
    " 296,\n",
    " 2571,\n",
    " 593,\n",
    " 110,\n",
    " 480,\n",
    " 260,\n",
    " 589,\n",
    " 527,\n",
    " 780,\n",
    " ...\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_counts = train_ratings.groupby(\"title\")[\"note\"].count().sort_values(ascending=False)\n",
    "popular_item_list = list(movie_counts.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3306\n"
     ]
    }
   ],
   "source": [
    "print(len(popular_item_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_items = set(popular_item_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Building the popular recommendation relevance list per user.\n",
    "\n",
    "To use our MRR metric function, we need to build a list of lists containing 0's or 1's. There is one list per user.\n",
    "0 means not relevant, 1 means relevant. We need to score each item until the first relevant one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_rel = []\n",
    "\n",
    "for user,will_see in ground_truth.items():\n",
    "    rel_list = []\n",
    "    will_see = set(will_see)\n",
    "    has_seen = set(already_seen.get(user, []))\n",
    "    \n",
    "    for movie in popular_item_list:\n",
    "        if movie in has_seen:         # User has already seen movie -> Can filter prediction\n",
    "            continue\n",
    "        elif movie in will_see:       # User will see, spot on suggestion !         \n",
    "            rel_list.append(1)\n",
    "            break\n",
    "        else:                         # No clue.\n",
    "            rel_list.append(0)\n",
    "            \n",
    "    if rel_list[-1] == 1:             # when no relevant item exist, no need to take it into account.\n",
    "        list_of_rel.append(rel_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On average, the 26th proposed item is relevant (on 3306)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_mrr = mrr(list_of_rel)\n",
    "f\"On average, the {int(round(1/pop_mrr,0))}th proposed item is relevant (on {len(existing_items)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our result is the following :\n",
    "> 'On average, the 4th proposed item is relevant (on 8970)'\n",
    "\n",
    "Ok, the popular baseline is WAY better than our SVD model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit Collaborative Filtering \n",
    "\n",
    "Implicit collaborative filtering, unlike explicit CF, is the task of learning from interactions (not ratings). The models are quite close but mainly differ by the optimized loss.\n",
    "\n",
    "\n",
    "\n",
    "Here, we propose to use the [lightFM](https://github.com/lyst/lightfm) model to do implicit collaborative filtering.\n",
    "\n",
    "\n",
    "> LightFM is a Python implementation of a number of popular recommendation algorithms for both implicit and explicit feedback, including efficient implementation of BPR and WARP ranking losses. It's easy to use, fast (via multithreaded model estimation), and produces high quality results.\n",
    "\n",
    "LightFM is a simple matrix factorization algorithm where a rating is predicted in the following way:\n",
    "\n",
    "$$\\hat{r_{ui}} = f(q_u · p_i + b_u + b_i)$$\n",
    "\n",
    "Where $q_u$,$p_i$ and $b_u$, $b_i$ are respectively user and item latent profile and features\n",
    "\n",
    "### What are interactions :\n",
    "\n",
    "Interactions can be any type (clicks, pause, gaze, comment, review) it's a lot more versatile than ratings. Here we'll consider two setups:\n",
    "\n",
    "- Interactions are ratings >= 5\n",
    "- Interactions are every ratings \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Let's create the interaction train/test dataset within the framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Relevant Documentation](http://lyst.github.io/lightfm/docs/lightfm.data.html)\n",
    "\n",
    "To create a dataset: \n",
    "- (a) Create an instance of the Dataset class.\n",
    "- (b) Call fit (or fit_partial), supplying user/item ids and feature names that you want to use in your model. This will create internal mappings that translate the ids and feature names to internal indices used by the LightFM model.\n",
    "- (c) Call build_interactions with an iterable of (user id, item id) or (user id, item id, weight) to build an interactions and weights matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hacho\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\lightfm\\_lightfm_fast.py:9: UserWarning: LightFM was compiled without OpenMP support. Only a single thread will be used.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from lightfm.data import Dataset\n",
    "\n",
    "# (a) Create a dataset\n",
    "dataset = Dataset()\n",
    "\n",
    "\n",
    "# (b) Create an internal mapping for users and items (We need to consider train + test)\n",
    "dataset.fit((x for x in ratings[\"author\"]),\n",
    "            (x for x in ratings['title']))\n",
    "\n",
    "# (c) Create the interaction matrices\n",
    "(train_interactions, weights) = dataset.build_interactions(\n",
    "    ((x.author, x.title) for x in train_ratings.itertuples() if x.note >= 8) # We only consider 5's as interactions\n",
    ") \n",
    "(test_interactions, weights) = dataset.build_interactions(\n",
    "    ((x.author, x.title) for x in test_ratings.itertuples() if x.note >= 8)  # We only consider 5's as interactions\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Create and train the lightFM model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model learns embeddings (latent representations in a high-dimensional space) for users and items in a way that encodes user preferences over items. When multiplied together, these representations produce scores for every item for a given user; items scored highly are more likely to be interesting to the user.\n",
    "\n",
    "Four loss functions are available:\n",
    "\n",
    "- logistic: useful when both positive (1) and negative (-1) interactions are present.\n",
    "- BPR: Bayesian Personalised Ranking [1] pairwise loss. Maximises the prediction difference between a positive example and a randomly chosen negative example. Useful when only positive interactions are present and optimising ROC AUC is desired.\n",
    "- WARP: Weighted Approximate-Rank Pairwise [2] loss. Maximises the rank of positive examples by repeatedly sampling negative examples until rank violating one is found. Useful when only positive interactions are present and optimising the top of the recommendation list (precision@k) is desired.\n",
    "- k-OS WARP: k-th order statistic loss [3]. A modification of WARP that uses the k-th positive example for any given user as a basis for pairwise updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from lightfm import LightFM\n",
    "\n",
    "model = LightFM(loss='bpr',random_state=50000)\n",
    "model.fit(train_interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Evaluation of learnt model with lightfm\n",
    "\n",
    "[Relevant documentation](http://lyst.github.io/lightfm/docs/lightfm.evaluation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Test interactions matrix and train interactions matrix share 23 interactions. This will cause incorrect evaluation, check your data split.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/martin/Library/Mobile Documents/com~apple~CloudDocs/L3/PROJET-RECHERCHE/RS2_corrected.ipynb Cell 51\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/martin/Library/Mobile%20Documents/com~apple~CloudDocs/L3/PROJET-RECHERCHE/RS2_corrected.ipynb#Y101sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlightfm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevaluation\u001b[39;00m \u001b[39mimport\u001b[39;00m reciprocal_rank\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/martin/Library/Mobile%20Documents/com~apple~CloudDocs/L3/PROJET-RECHERCHE/RS2_corrected.ipynb#Y101sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m bpr_mrr \u001b[39m=\u001b[39m reciprocal_rank(model, test_interactions, train_interactions)\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/lightfm/evaluation.py:311\u001b[0m, in \u001b[0;36mreciprocal_rank\u001b[0;34m(model, test_interactions, train_interactions, user_features, item_features, preserve_rows, num_threads, check_intersections)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mif\u001b[39;00m num_threads \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    309\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNumber of threads must be 1 or larger.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 311\u001b[0m ranks \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict_rank(\n\u001b[1;32m    312\u001b[0m     test_interactions,\n\u001b[1;32m    313\u001b[0m     train_interactions\u001b[39m=\u001b[39;49mtrain_interactions,\n\u001b[1;32m    314\u001b[0m     user_features\u001b[39m=\u001b[39;49muser_features,\n\u001b[1;32m    315\u001b[0m     item_features\u001b[39m=\u001b[39;49mitem_features,\n\u001b[1;32m    316\u001b[0m     num_threads\u001b[39m=\u001b[39;49mnum_threads,\n\u001b[1;32m    317\u001b[0m     check_intersections\u001b[39m=\u001b[39;49mcheck_intersections,\n\u001b[1;32m    318\u001b[0m )\n\u001b[1;32m    320\u001b[0m ranks\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m (ranks\u001b[39m.\u001b[39mdata \u001b[39m+\u001b[39m \u001b[39m1.0\u001b[39m)\n\u001b[1;32m    322\u001b[0m ranks \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqueeze(np\u001b[39m.\u001b[39marray(ranks\u001b[39m.\u001b[39mmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtodense()))\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/lightfm/lightfm.py:945\u001b[0m, in \u001b[0;36mLightFM.predict_rank\u001b[0;34m(self, test_interactions, train_interactions, item_features, user_features, num_threads, check_intersections)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNumber of threads must be 1 or larger.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    944\u001b[0m \u001b[39mif\u001b[39;00m check_intersections:\n\u001b[0;32m--> 945\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_test_train_intersections(test_interactions, train_interactions)\n\u001b[1;32m    947\u001b[0m n_users, n_items \u001b[39m=\u001b[39m test_interactions\u001b[39m.\u001b[39mshape\n\u001b[1;32m    949\u001b[0m (user_features, item_features) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_feature_matrices(\n\u001b[1;32m    950\u001b[0m     n_users, n_items, user_features, item_features\n\u001b[1;32m    951\u001b[0m )\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/lightfm/lightfm.py:878\u001b[0m, in \u001b[0;36mLightFM._check_test_train_intersections\u001b[0;34m(self, test_mat, train_mat)\u001b[0m\n\u001b[1;32m    876\u001b[0m n_intersections \u001b[39m=\u001b[39m test_mat\u001b[39m.\u001b[39mmultiply(train_mat)\u001b[39m.\u001b[39mnnz\n\u001b[1;32m    877\u001b[0m \u001b[39mif\u001b[39;00m n_intersections:\n\u001b[0;32m--> 878\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    879\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTest interactions matrix and train interactions \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmatrix share \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m interactions. This will cause \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    881\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mincorrect evaluation, check your data split.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m n_intersections\n\u001b[1;32m    882\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Test interactions matrix and train interactions matrix share 23 interactions. This will cause incorrect evaluation, check your data split."
     ]
    }
   ],
   "source": [
    "from lightfm.evaluation import reciprocal_rank\n",
    "bpr_mrr = reciprocal_rank(model, test_interactions, train_interactions).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"On average, the {int(round(1/bpr_mrr,0))}th proposed item is relevant (on {len(existing_items)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we get\n",
    "> 'On average, the 6th proposed item is relevant (on 8970)'\n",
    "\n",
    "It's much better than the SVD, but still worse than the popular baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Let's do the same, but now we consider EVERY rating as one interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Test interactions matrix and train interactions matrix share 40 interactions. This will cause incorrect evaluation, check your data split.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/martin/Library/Mobile Documents/com~apple~CloudDocs/L3/PROJET-RECHERCHE/RS2_corrected.ipynb Cell 55\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/martin/Library/Mobile%20Documents/com~apple~CloudDocs/L3/PROJET-RECHERCHE/RS2_corrected.ipynb#Y105sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model_bpr_all \u001b[39m=\u001b[39m LightFM(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbpr\u001b[39m\u001b[39m'\u001b[39m,random_state\u001b[39m=\u001b[39m\u001b[39m50000\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/martin/Library/Mobile%20Documents/com~apple~CloudDocs/L3/PROJET-RECHERCHE/RS2_corrected.ipynb#Y105sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model_bpr_all\u001b[39m.\u001b[39mfit(train_interactions_all)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/martin/Library/Mobile%20Documents/com~apple~CloudDocs/L3/PROJET-RECHERCHE/RS2_corrected.ipynb#Y105sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m bpr_mrr_all \u001b[39m=\u001b[39m reciprocal_rank(model_bpr_all, test_interactions_all, train_interactions_all)\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/lightfm/evaluation.py:311\u001b[0m, in \u001b[0;36mreciprocal_rank\u001b[0;34m(model, test_interactions, train_interactions, user_features, item_features, preserve_rows, num_threads, check_intersections)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mif\u001b[39;00m num_threads \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    309\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNumber of threads must be 1 or larger.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 311\u001b[0m ranks \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict_rank(\n\u001b[1;32m    312\u001b[0m     test_interactions,\n\u001b[1;32m    313\u001b[0m     train_interactions\u001b[39m=\u001b[39;49mtrain_interactions,\n\u001b[1;32m    314\u001b[0m     user_features\u001b[39m=\u001b[39;49muser_features,\n\u001b[1;32m    315\u001b[0m     item_features\u001b[39m=\u001b[39;49mitem_features,\n\u001b[1;32m    316\u001b[0m     num_threads\u001b[39m=\u001b[39;49mnum_threads,\n\u001b[1;32m    317\u001b[0m     check_intersections\u001b[39m=\u001b[39;49mcheck_intersections,\n\u001b[1;32m    318\u001b[0m )\n\u001b[1;32m    320\u001b[0m ranks\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m (ranks\u001b[39m.\u001b[39mdata \u001b[39m+\u001b[39m \u001b[39m1.0\u001b[39m)\n\u001b[1;32m    322\u001b[0m ranks \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqueeze(np\u001b[39m.\u001b[39marray(ranks\u001b[39m.\u001b[39mmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtodense()))\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/lightfm/lightfm.py:945\u001b[0m, in \u001b[0;36mLightFM.predict_rank\u001b[0;34m(self, test_interactions, train_interactions, item_features, user_features, num_threads, check_intersections)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNumber of threads must be 1 or larger.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    944\u001b[0m \u001b[39mif\u001b[39;00m check_intersections:\n\u001b[0;32m--> 945\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_test_train_intersections(test_interactions, train_interactions)\n\u001b[1;32m    947\u001b[0m n_users, n_items \u001b[39m=\u001b[39m test_interactions\u001b[39m.\u001b[39mshape\n\u001b[1;32m    949\u001b[0m (user_features, item_features) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_feature_matrices(\n\u001b[1;32m    950\u001b[0m     n_users, n_items, user_features, item_features\n\u001b[1;32m    951\u001b[0m )\n",
      "File \u001b[0;32m~/micromamba/lib/python3.9/site-packages/lightfm/lightfm.py:878\u001b[0m, in \u001b[0;36mLightFM._check_test_train_intersections\u001b[0;34m(self, test_mat, train_mat)\u001b[0m\n\u001b[1;32m    876\u001b[0m n_intersections \u001b[39m=\u001b[39m test_mat\u001b[39m.\u001b[39mmultiply(train_mat)\u001b[39m.\u001b[39mnnz\n\u001b[1;32m    877\u001b[0m \u001b[39mif\u001b[39;00m n_intersections:\n\u001b[0;32m--> 878\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    879\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTest interactions matrix and train interactions \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmatrix share \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m interactions. This will cause \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    881\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mincorrect evaluation, check your data split.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m n_intersections\n\u001b[1;32m    882\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Test interactions matrix and train interactions matrix share 40 interactions. This will cause incorrect evaluation, check your data split."
     ]
    }
   ],
   "source": [
    "# Create the interaction matrix\n",
    "(train_interactions_all, weights) = dataset.build_interactions(((x.author, x.title) for x in train_ratings.itertuples())) # We consider every interactions\n",
    "(test_interactions_all, weights) = dataset.build_interactions(((x.author, x.title) for x in test_ratings.itertuples())) # We consider every interactions\n",
    "\n",
    "from lightfm import LightFM\n",
    "\n",
    "#\n",
    "model_bpr_all = LightFM(loss='bpr',random_state=50000)\n",
    "model_bpr_all.fit(train_interactions_all)\n",
    "\n",
    "bpr_mrr_all = reciprocal_rank(model_bpr_all, test_interactions_all, train_interactions_all).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"On average, the {int(round(1/bpr_mrr_all,0))}th proposed item is relevant (on {len(existing_items)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we got\n",
    "> 'On average, the 3th proposed item is relevant (on 8970)'\n",
    "\n",
    "Much better !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some time left ? Try and experiment with every lightfm losses "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
