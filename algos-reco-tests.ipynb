{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALGOS DE RECO #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# on importe les differentes librairies\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# on importe les differentes librairies\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy import stats\n",
    "\n",
    "# on importe les différentes librairies surprise de scikit\n",
    "from surprise import SVD\n",
    "from surprise import dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.prediction_algorithms.knns import KNNBasic, KNNWithMeans\n",
    "\n",
    "# on importe notre fonction de tri\n",
    "from tri_threshold import filter_reviews\n",
    "\n",
    "# on importe notre bdd\n",
    "avis = pd.read_csv(\"BDD/avis.csv\", index_col=\"Unnamed: 0\")\n",
    "avis_norm = pd.read_csv(\"BDD/avis_norm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date_published</th>\n",
       "      <th>title_review</th>\n",
       "      <th>note</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monsieur Guillaume</td>\n",
       "      <td>2021-01</td>\n",
       "      <td>Voyages sur les ailes des papillons</td>\n",
       "      <td>8</td>\n",
       "      <td>Mariposas</td>\n",
       "      <td>https://www.trictrac.net/jeu-de-societe/maripo...</td>\n",
       "      <td>Lorsque le jeu est jeu, bon, réflexif, joli po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author date_published                         title_review  \\\n",
       "0  Monsieur Guillaume        2021-01  Voyages sur les ailes des papillons   \n",
       "\n",
       "   note      title                                                url  \\\n",
       "0     8  Mariposas  https://www.trictrac.net/jeu-de-societe/maripo...   \n",
       "\n",
       "                                             comment  \n",
       "0  Lorsque le jeu est jeu, bon, réflexif, joli po...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avis.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               author date_published  \\\n",
      "619            Zypher        2012-12   \n",
      "620          Manu0801        2018-02   \n",
      "704             api25        2016-06   \n",
      "1368           Olives        2019-11   \n",
      "2186           apiret        2019-12   \n",
      "...               ...            ...   \n",
      "174110          DidOo        2011-12   \n",
      "174366          DidOo        2014-09   \n",
      "174397           CyrC        2009-06   \n",
      "174639  the_farbarian        2016-06   \n",
      "176064          Albes        2016-11   \n",
      "\n",
      "                                             title_review  note  \\\n",
      "619                                 Fuyez, pauvres fous !     0   \n",
      "620     Long et incontrôlable, ça existe encore ce typ...     0   \n",
      "704                                      Long et ennuyeux     0   \n",
      "1368                                     Vraiment mauvais     0   \n",
      "2186                                     Vraiment mauvais     0   \n",
      "...                                                   ...   ...   \n",
      "174110                           Pour vous la raconter...     0   \n",
      "174366                               Un bon gros dodo ...     0   \n",
      "174397                                    Kado not for me     0   \n",
      "174639                                   Vraiment mauvais     0   \n",
      "176064                                     quelle erreur!     0   \n",
      "\n",
      "                                title  \\\n",
      "619     Talisman - 4e Édition Révisée   \n",
      "620     Talisman - 4e Édition Révisée   \n",
      "704     Talisman - 4e Édition Révisée   \n",
      "1368                 6 qui surprend !   \n",
      "2186           It's a Wonderful World   \n",
      "...                               ...   \n",
      "174110                         Échecs   \n",
      "174366                         Mikado   \n",
      "174397                         Mikado   \n",
      "174639                 Rum & Bones VF   \n",
      "176064                  Anti-Monopoly   \n",
      "\n",
      "                                                      url  \\\n",
      "619     https://www.trictrac.net/jeu-de-societe/talism...   \n",
      "620     https://www.trictrac.net/jeu-de-societe/talism...   \n",
      "704     https://www.trictrac.net/jeu-de-societe/talism...   \n",
      "1368    https://www.trictrac.net/jeu-de-societe/6-qui-...   \n",
      "2186    https://www.trictrac.net/jeu-de-societe/its-a-...   \n",
      "...                                                   ...   \n",
      "174110  https://www.trictrac.net/jeu-de-societe/echecs...   \n",
      "174366  https://www.trictrac.net/jeu-de-societe/mikado...   \n",
      "174397  https://www.trictrac.net/jeu-de-societe/mikado...   \n",
      "174639  https://www.trictrac.net/jeu-de-societe/rum-bo...   \n",
      "176064  https://www.trictrac.net/jeu-de-societe/anti-m...   \n",
      "\n",
      "                                                  comment  \n",
      "619     Avec ses figurines, son grand plateau, ses cen...  \n",
      "620     Supplice!\\nCe jeu n'est vraiment pas pour moi,...  \n",
      "704     Talisman est un jeu de plateau s'apparentant à...  \n",
      "1368      Incontrolable à 6 joueurs ! On subit la partie.  \n",
      "2186                                                  NaN  \n",
      "...                                                   ...  \n",
      "174110  Ben oui; car si on veut briller en société, c'...  \n",
      "174366  Un jeu d'adresse archi connu donc on va pas s'...  \n",
      "174397  Au moins, il ne tient pas de placeMais décidem...  \n",
      "174639                                                NaN  \n",
      "176064  quelle erreur de l'avoir acheté ce jeu!\\nargen...  \n",
      "\n",
      "[374 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sélection des lignes où la note est égale à 0\n",
    "jeux_avec_note_zero = avis.loc[avis['note'] == 0]\n",
    "\n",
    "# Affichage des résultats\n",
    "print(jeux_avec_note_zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut mettre en place la matrice sparse (calculer la sparsité + plot cette sparsite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il y'a 2564 auteurs et 3057 jeux\n",
      "Notre matrice a une sparsité de 1.60% \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x125299 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 124925 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users  = avis[\"author\"].nunique()\n",
    "num_items  = avis[\"title\"].nunique()\n",
    "\n",
    "print(f\"Il y'a {num_users} auteurs et {num_items} jeux\")\n",
    "\n",
    "sparsity = (len(avis) / (num_users*num_items))*100\n",
    "print(f\"Notre matrice a une sparsité de {sparsity:.2f}% \")\n",
    "\n",
    "matrice_sparse = csr_matrix(avis['note'])\n",
    "matrice_sparse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On test maintenant les differents algos (SVD + KNN pour commencer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.8302  1.8393  1.8369  1.8300  1.8236  1.8320  0.0056  \n",
      "MAE (testset)     1.3995  1.4114  1.4092  1.4042  1.4058  1.4060  0.0041  \n",
      "Fit time          3.23    3.04    3.40    3.40    3.19    3.25    0.13    \n",
      "Test time         0.11    0.17    0.12    0.11    0.17    0.14    0.03    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([1.83020469, 1.83930214, 1.83689914, 1.8299808 , 1.82359466]),\n",
       " 'test_mae': array([1.39953742, 1.41141878, 1.40921506, 1.40424574, 1.40579265]),\n",
       " 'fit_time': (3.229295492172241,\n",
       "  3.0393242835998535,\n",
       "  3.3971614837646484,\n",
       "  3.3953378200531006,\n",
       "  3.1927309036254883),\n",
       " 'test_time': (0.10840439796447754,\n",
       "  0.17353558540344238,\n",
       "  0.11569046974182129,\n",
       "  0.1077117919921875,\n",
       "  0.17322993278503418)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = SVD(n_factors=50,n_epochs=20,lr_all=0.005,reg_all=0.05)\n",
    "\n",
    "reader = Reader(line_format='user item rating', rating_scale=(0,10))\n",
    "\n",
    "class MyDataset(dataset.DatasetAutoFolds):\n",
    "    def __init__(self, df, reader):\n",
    "        self.raw_ratings = [(uid, iid, r, None) for (uid, iid, r) in\n",
    "                            zip(df['author'], df['title'], df['note'])]\n",
    "        self.reader = reader\n",
    "\n",
    "\n",
    "data_test = MyDataset(avis, reader)\n",
    "cross_validate(algo, data_test, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.1944  0.1928  0.1923  0.1925  0.1945  0.1933  0.0010  \n",
      "MAE (testset)     0.1504  0.1487  0.1485  0.1491  0.1508  0.1495  0.0009  \n",
      "Fit time          5.62    5.28    5.24    5.30    5.36    5.36    0.13    \n",
      "Test time         0.11    0.18    0.11    0.18    0.13    0.14    0.03    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([0.194417  , 0.1927808 , 0.1922522 , 0.19250135, 0.19447668]),\n",
       " 'test_mae': array([0.15043278, 0.14868858, 0.14852469, 0.14910759, 0.15080227]),\n",
       " 'fit_time': (5.615259647369385,\n",
       "  5.276290416717529,\n",
       "  5.242383241653442,\n",
       "  5.304482698440552,\n",
       "  5.364598035812378),\n",
       " 'test_time': (0.11465334892272949,\n",
       "  0.17752504348754883,\n",
       "  0.11369609832763672,\n",
       "  0.17952609062194824,\n",
       "  0.13364267349243164)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Reader object specifying the rating scale\n",
    "reader = Reader(line_format='user item rating', rating_scale=(0,1))\n",
    "\n",
    "# Custom Dataset class to load our dataset\n",
    "class MyDataset(dataset.DatasetAutoFolds):\n",
    "    def __init__(self, df, reader):\n",
    "        self.raw_ratings = [(uid, iid, r, None) for (uid, iid, r) in\n",
    "                            zip(df['author'], df['title'], df['note'])]\n",
    "        self.reader = reader\n",
    "\n",
    "# Instantiate the dataset with your data\n",
    "data = MyDataset(avis_norm, reader)\n",
    "\n",
    "# Initialize the SVD algorithm\n",
    "algo = SVD()\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'avis' is your DataFrame and you're analyzing the 'note' column for outliers\n",
    "z_scores = stats.zscore(avis['note'])\n",
    "abs_z_scores = abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 2)  # Adjust the threshold as necessary\n",
    "cleaned_data = avis[filtered_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    1.5532  1.5596  1.5363  1.5376  1.5429  1.5459  0.0091  \n",
      "MAE (testset)     1.2363  1.2391  1.2258  1.2266  1.2280  1.2312  0.0055  \n",
      "Fit time          3.07    3.03    2.90    2.98    3.28    3.05    0.13    \n",
      "Test time         0.18    0.11    0.11    0.10    0.11    0.12    0.03    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([1.55319331, 1.55960379, 1.53629005, 1.53755461, 1.54288815]),\n",
       " 'test_mae': array([1.23631652, 1.23906095, 1.22576629, 1.22660032, 1.22802834]),\n",
       " 'fit_time': (3.066718816757202,\n",
       "  3.030341863632202,\n",
       "  2.897665023803711,\n",
       "  2.9796957969665527,\n",
       "  3.2814018726348877),\n",
       " 'test_time': (0.17548632621765137,\n",
       "  0.11468982696533203,\n",
       "  0.10523509979248047,\n",
       "  0.10272359848022461,\n",
       "  0.11417174339294434)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Reader object specifying the rating scale\n",
    "reader = Reader(line_format='user item rating', rating_scale=(0,10))\n",
    "\n",
    "# Custom Dataset class to load our dataset\n",
    "class MyDataset(dataset.DatasetAutoFolds):\n",
    "    def __init__(self, df, reader):\n",
    "        self.raw_ratings = [(uid, iid, r, None) for (uid, iid, r) in\n",
    "                            zip(df['author'], df['title'], df['note'])]\n",
    "        self.reader = reader\n",
    "\n",
    "# Instantiate the dataset with your data\n",
    "data_clean = MyDataset(cleaned_data, reader)\n",
    "\n",
    "# Initialize the SVD algorithm\n",
    "algo = SVD(n_factors=20,n_epochs=30,lr_all=0.005,reg_all=0.2)\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_validate(algo, data_clean, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_factors': [50, 100, 150],  # Number of factors\n",
    "    'n_epochs': [20, 30],         # Number of iterations\n",
    "    'lr_all': [0.005, 0.01],      # Learning rate\n",
    "    'reg_all': [0.02, 0.05]       # Regularization term\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "gs.fit(data_test)\n",
    "\n",
    "# Best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# Combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_factors': [20, 50, 100, 150, 200],  # More options for the number of factors\n",
    "    'n_epochs': [5, 10, 20, 30, 50],       # Broader range of iterations\n",
    "    'lr_all': [0.002, 0.005, 0.01, 0.02],  # Wider range of learning rates\n",
    "    'reg_all': [0.02, 0.05, 0.1, 0.2],     # Wider range of regularization terms\n",
    "    'biased': [True, False],               # Whether to use baseline factors or not\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "\n",
    "# Fit GridSearchCV\n",
    "gs.fit(data_clean)\n",
    "\n",
    "# Best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# Combination of parameters that gave the best RMSE score\n",
    "print(gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_processed = filter_reviews(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data_processed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Reader object specifying the rating scale\n",
    "reader = Reader(line_format='user item rating', rating_scale=(0,10))\n",
    "\n",
    "# Custom Dataset class to load our dataset\n",
    "class MyDataset(dataset.DatasetAutoFolds):\n",
    "    def __init__(self, df, reader):\n",
    "        self.raw_ratings = [(uid, iid, r, None) for (uid, iid, r) in\n",
    "                            zip(df['author'], df['title'], df['note'])]\n",
    "        self.reader = reader\n",
    "\n",
    "# Instantiate the dataset with your data\n",
    "data_clean_proc = MyDataset(cleaned_data_processed, reader)\n",
    "\n",
    "# Initialize the SVD algorithm\n",
    "algo = SVD(n_factors=20,n_epochs=30,lr_all=0.005,reg_all=0.2)\n",
    "\n",
    "# Perform cross-validation\n",
    "cross_validate(algo, data_clean_proc, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-NN ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = KNNBasic()\n",
    "\n",
    "reader = Reader(line_format='user item rating', rating_scale=(0,10))\n",
    "\n",
    "class MyDataset(dataset.DatasetAutoFolds):\n",
    "\n",
    "    def __init__(self, jeux, reader):\n",
    "\n",
    "        self.raw_ratings = [(uid, iid, r, None) for (uid, iid, r) in\n",
    "                            zip(avis['author'], avis['title'], avis['note'])]\n",
    "        self.reader=reader\n",
    "\n",
    "data = MyDataset(jeux, reader)\n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avis_norm.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
